{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'what', 'is', 'the', 'second', 'largest', 'city', 'in', 'the', 'japan', '?', '[SEP]']\n",
      "['it', 'is', '[MASK]', '.', '[SEP]']\n",
      "['[CLS]', 'what', 'is', 'the', 'second', 'largest', 'city', 'in', 'the', 'japan', '?', '[SEP]', 'it', 'is', '[MASK]', '.', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertForMaskedLM\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "question = \"[CLS] What is the second largest city in the japan ? [SEP]\"\n",
    "answer = \" It is [MASK] . [SEP]\"\n",
    "query = question + answer\n",
    "\n",
    "tokenized_question = tokenizer.tokenize(question)\n",
    "tokenized_answer = tokenizer.tokenize(answer)\n",
    "tokenized_query = tokenizer.tokenize(query)\n",
    "\n",
    "print(tokenized_question)\n",
    "print(tokenized_answer)\n",
    "print(tokenized_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 2054, 2003, 1996, 2117, 2922, 2103, 1999, 1996, 2900, 1029, 102, 2009, 2003, 103, 1012, 102]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_query)\n",
    "segments_ids = [0] * len(tokenized_question) + [1] * len(tokenized_answer)\n",
    "\n",
    "print(indexed_tokens)\n",
    "print(segments_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beautiful\n"
     ]
    }
   ],
   "source": [
    "tokens_tensor = torch.tensor([indexed_tokens])\n",
    "segments_tensors = torch.tensor([segments_ids])\n",
    "\n",
    "model = BertForMaskedLM.from_pretrained('bert-base-uncased')\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    predictions = model(tokens_tensor, segments_tensors)\n",
    "    \n",
    "masked_index = tokenized_query.index(\"[MASK]\")\n",
    "predicted_index = torch.argmax(predictions[0, masked_index]).item()\n",
    "predicted_token = tokenizer.convert_ids_to_tokens([predicted_index])[0]\n",
    "\n",
    "print(predicted_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
